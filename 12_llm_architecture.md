---
**Voice Assistant - Computer Wake-Word Project**  
**Dokument:** LLM-Integration Architektur  
**Datum:** 05. Dezember 2025  
**Seite:** {page}
---

# LLM-Integration Architektur
## Planung f√ºr intelligente Konversationen

---

## Inhaltsverzeichnis

1. [√úbersicht](#√ºbersicht)
2. [Warum LLM-Integration?](#warum)
3. [Architektur-Design](#architektur)
4. [LLM-Auswahl](#llm-auswahl)
5. [Command vs. Question Classification](#classification)
6. [API-Integration](#api-integration)
7. [Fallback-Strategie](#fallback)
8. [Code-Struktur](#code-struktur)
9. [Implementierungs-Roadmap](#roadmap)

---

## √úbersicht {#√ºbersicht}

Die LLM-Integration erweitert den Voice Assistant von einem einfachen Befehlsausf√ºhrungs-System zu einem intelligenten Konversations-Partner. Statt nur vordefinierte Befehle auszuf√ºhren, kann der Assistant komplexe Fragen beantworten, Recherchen durchf√ºhren und kontextbasierte Gespr√§che f√ºhren.

### Aktueller Zustand (v3.0)

Der Voice Assistant funktioniert derzeit nach einem einfachen Pattern-Matching-Prinzip:

```
User: "Computer, √∂ffne YouTube"
  ‚Üí Pattern erkannt: "youtube" in command_lower
  ‚Üí Aktion: webbrowser.open("https://www.youtube.com")
  ‚Üí Response: "√ñffne YouTube"
```

**Limitierungen:**
- Nur vordefinierte Befehle funktionieren
- Keine Fragen beantwortbar ("Wie wird das Wetter?")
- Keine Konversation m√∂glich
- Keine Kontextspeicherung

### Ziel-Zustand (v4.0 mit LLM)

Mit LLM-Integration wird der Assistant intelligent:

```
User: "Computer, wie wird das Wetter morgen in Berlin?"
  ‚Üí Klassifizierung: FRAGE (kein Befehl)
  ‚Üí LLM-Aufruf: Perplexity API
  ‚Üí Response: "Morgen wird es in Berlin sonnig mit 18 Grad..."
  
User: "Computer, schreibe eine E-Mail an Max √ºber das Meeting"
  ‚Üí Klassifizierung: KOMPLEXE AUFGABE
  ‚Üí LLM-Aufruf: ChatGPT API
  ‚Üí Response: "Ich habe einen E-Mail-Entwurf erstellt: ..."
```

---

## Warum LLM-Integration? {#warum}

### Vorteile

**Nat√ºrliche Konversation:** Nutzer k√∂nnen in nat√ºrlicher Sprache sprechen, ohne sich an spezifische Befehlsformate erinnern zu m√ºssen. Statt "√∂ffne YouTube" kann man fragen "Kannst du mir YouTube √∂ffnen?" oder "Ich m√∂chte Videos schauen".

**Intelligente Antworten:** Der Assistant kann komplexe Fragen beantworten, die nicht vorprogrammiert sind. Wetter-Abfragen, Fakten-Recherche, Berechnungen, √úbersetzungen und vieles mehr werden m√∂glich.

**Kontext-Verst√§ndnis:** LLMs k√∂nnen den Kontext eines Gespr√§chs verstehen und darauf aufbauen. Folgefragen wie "Und √ºbermorgen?" werden korrekt interpretiert.

**Erweiterbarkeit:** Neue Funktionen k√∂nnen durch Prompt-Engineering hinzugef√ºgt werden, ohne Code zu √§ndern. Der Assistant lernt durch bessere Prompts.

**Personalisierung:** LLMs k√∂nnen sich an Nutzerpr√§ferenzen anpassen und personalisierte Antworten geben.

### Herausforderungen

**Latenz:** API-Aufrufe zu externen LLMs dauern 1-5 Sekunden, was die Reaktionszeit erh√∂ht.

**Kosten:** ChatGPT und andere APIs sind kostenpflichtig (ca. $0.001-0.01 pro Anfrage).

**Internetabh√§ngigkeit:** Ohne Internet funktionieren Cloud-LLMs nicht.

**Datenschutz:** Audio/Text wird an externe Server gesendet (OpenAI, Perplexity, etc.).

**Zuverl√§ssigkeit:** API-Ausf√§lle oder Rate-Limits k√∂nnen die Funktionalit√§t beeintr√§chtigen.

---

## Architektur-Design {#architektur}

### High-Level Architektur

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    USER INPUT                           ‚îÇ
‚îÇ              "Computer, [command/question]"             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              WAKE-WORD DETECTION                        ‚îÇ
‚îÇ                  (Porcupine)                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              SPEECH-TO-TEXT                             ‚îÇ
‚îÇ                   (Vosk)                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          COMMAND vs. QUESTION CLASSIFIER                ‚îÇ
‚îÇ         (Pattern Matching + LLM Fallback)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                       ‚îÇ
         ‚ñº                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  COMMAND ENGINE  ‚îÇ    ‚îÇ   LLM ROUTER     ‚îÇ
‚îÇ  (Local)         ‚îÇ    ‚îÇ  (API Calls)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ
         ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ              ‚îÇ                 ‚îÇ
         ‚îÇ              ‚ñº                 ‚ñº
         ‚îÇ      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ      ‚îÇ   ChatGPT    ‚îÇ  ‚îÇ  Perplexity  ‚îÇ
         ‚îÇ      ‚îÇ  (General)   ‚îÇ  ‚îÇ  (Research)  ‚îÇ
         ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ             ‚îÇ                 ‚îÇ
         ‚îÇ             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                      ‚îÇ
         ‚ñº                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              RESPONSE AGGREGATOR                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              TEXT-TO-SPEECH                             ‚îÇ
‚îÇ                 (Edge TTS)                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 AUDIO OUTPUT                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Komponenten-Beschreibung

**Wake-Word Detection:** Aktiviert den Assistant (bereits implementiert mit Porcupine).

**Speech-to-Text:** Konvertiert Sprache zu Text (bereits implementiert mit Vosk).

**Classifier:** Entscheidet, ob Input ein Befehl oder eine Frage ist. Dies ist die zentrale neue Komponente.

**Command Engine:** F√ºhrt lokale Befehle aus (bereits implementiert).

**LLM Router:** W√§hlt das passende LLM basierend auf Anfrage-Typ.

**LLM APIs:** ChatGPT f√ºr allgemeine Fragen, Perplexity f√ºr Recherche, etc.

**Response Aggregator:** Kombiniert Antworten und formatiert sie f√ºr TTS.

**Text-to-Speech:** Spricht die Antwort (bereits implementiert mit Edge TTS).

---

## LLM-Auswahl {#llm-auswahl}

### ChatGPT (OpenAI)

**Anwendungsfall:** Allgemeine Konversation, Kreativit√§t, Textgenerierung

**St√§rken:**
- Sehr gute nat√ºrliche Sprache
- Kontextverst√§ndnis
- Kreative Antworten
- Multimodal (Text, Code, Reasoning)

**Schw√§chen:**
- Wissen nur bis Trainings-Cutoff (2023)
- Keine Echtzeit-Informationen
- Kann "halluzinieren" (falsche Fakten)

**Kosten:**
- GPT-4o: $0.005/1K input tokens, $0.015/1K output tokens
- GPT-4o-mini: $0.00015/1K input tokens, $0.0006/1K output tokens

**API-Beispiel:**
```python
from openai import OpenAI
client = OpenAI(api_key="sk-...")

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "Du bist ein hilfreicher Voice Assistant."},
        {"role": "user", "content": "Wie wird das Wetter morgen?"}
    ]
)
answer = response.choices[0].message.content
```

**Empfehlung:** Nutze f√ºr allgemeine Fragen, Kreativit√§t, E-Mail-Entw√ºrfe, Zusammenfassungen.

---

### Perplexity

**Anwendungsfall:** Recherche, aktuelle Informationen, Fakten-Check

**St√§rken:**
- Echtzeit-Websuche
- Zitiert Quellen
- Aktuelle Informationen (Wetter, News, etc.)
- Fakten-basiert (weniger Halluzinationen)

**Schw√§chen:**
- Weniger kreativ als ChatGPT
- Fokus auf Fakten, nicht Konversation
- Teurer als ChatGPT

**Kosten:**
- Sonar (Standard): $0.005/1K tokens
- Sonar Pro: $0.01/1K tokens

**API-Beispiel:**
```python
import requests

url = "https://api.perplexity.ai/chat/completions"
headers = {
    "Authorization": "Bearer pplx-...",
    "Content-Type": "application/json"
}
data = {
    "model": "sonar",
    "messages": [
        {"role": "user", "content": "Wie wird das Wetter morgen in Berlin?"}
    ]
}

response = requests.post(url, headers=headers, json=data)
answer = response.json()["choices"][0]["message"]["content"]
```

**Empfehlung:** Nutze f√ºr Wetter, News, Fakten-Recherche, aktuelle Events.

---

### Manus

**Anwendungsfall:** Komplexe Aufgaben, Multi-Step-Reasoning, Tool-Use

**St√§rken:**
- Kann Tools nutzen (Browser, Code, etc.)
- Multi-Step-Reasoning
- Komplexe Aufgaben automatisieren
- Sehr gutes Verst√§ndnis

**Schw√§chen:**
- Langsamer (mehrere Sekunden)
- Komplexere Integration
- Nicht f√ºr einfache Fragen geeignet

**Kosten:** Abh√§ngig von Manus-Subscription

**Empfehlung:** Nutze f√ºr komplexe Aufgaben wie "Erstelle eine Pr√§sentation √ºber KI" oder "Recherchiere und fasse zusammen".

---

### Lokale LLMs (Optional)

**Anwendungsfall:** Offline-Nutzung, Datenschutz

**Optionen:**
- Ollama (Llama 3, Mistral, etc.)
- LM Studio
- GPT4All

**St√§rken:**
- Vollst√§ndig offline
- Keine API-Kosten
- Datenschutz (nichts verl√§sst PC)

**Schw√§chen:**
- Ben√∂tigt leistungsstarke GPU (8GB+ VRAM)
- Langsamer als Cloud-LLMs
- Weniger intelligent als GPT-4

**Empfehlung:** F√ºr zuk√ºnftige Version als Fallback implementieren.

---

## Command vs. Question Classification {#classification}

### Klassifizierungs-Strategie

Die Klassifizierung entscheidet, ob ein Input ein **Befehl** (lokal ausf√ºhrbar) oder eine **Frage** (LLM-Anfrage) ist.

### Methode 1: Pattern-Based (Schnell, Offline)

**Befehl-Patterns:**
```python
COMMAND_KEYWORDS = [
    "√∂ffne", "starte", "schlie√üe", "zeige", "spiele",
    "√∂ffnen", "starten", "schlie√üen", "zeigen", "spielen",
    "mach", "mache", "erstelle", "l√∂sche"
]

def is_command(text):
    text_lower = text.lower()
    for keyword in COMMAND_KEYWORDS:
        if keyword in text_lower:
            return True
    return False
```

**Frage-Patterns:**
```python
QUESTION_KEYWORDS = [
    "wie", "was", "wer", "wann", "wo", "warum", "welche",
    "erkl√§re", "sage mir", "erz√§hle", "beschreibe"
]

def is_question(text):
    text_lower = text.lower()
    for keyword in QUESTION_KEYWORDS:
        if text_lower.startswith(keyword):
            return True
    if "?" in text:
        return True
    return False
```

**Vorteile:** Schnell, offline, keine API-Kosten
**Nachteile:** Ungenau, viele Edge-Cases

---

### Methode 2: LLM-Based Classification (Genau, Online)

Nutze ein kleines, schnelles LLM (GPT-4o-mini) zur Klassifizierung:

```python
def classify_intent(text):
    """Klassifiziert Input als COMMAND, QUESTION oder CONVERSATION."""
    
    prompt = f"""Klassifiziere folgende Nutzer-Eingabe:

Input: "{text}"

Kategorien:
- COMMAND: Befehl zum Ausf√ºhren einer Aktion (√∂ffne, starte, etc.)
- QUESTION: Frage, die eine Antwort ben√∂tigt (wie, was, wann, etc.)
- CONVERSATION: Smalltalk oder H√∂flichkeit (hallo, danke, etc.)

Antworte nur mit: COMMAND, QUESTION oder CONVERSATION"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=10,
        temperature=0
    )
    
    classification = response.choices[0].message.content.strip()
    return classification
```

**Vorteile:** Sehr genau, versteht Kontext
**Nachteile:** Latenz (+500ms), API-Kosten

---

### Methode 3: Hybrid (Empfohlen)

Kombiniere beide Methoden:

```python
def classify_input(text):
    """Hybrid-Klassifizierung: Pattern-based first, LLM fallback."""
    
    # 1. Schnelle Pattern-Checks
    if is_command(text):
        return "COMMAND"
    
    if is_question(text):
        return "QUESTION"
    
    # 2. Spezielle F√§lle
    if any(word in text.lower() for word in ["danke", "hallo", "tsch√ºss"]):
        return "CONVERSATION"
    
    # 3. LLM-Fallback f√ºr unklare F√§lle
    return classify_intent(text)  # LLM-Aufruf
```

**Vorteile:** Schnell f√ºr klare F√§lle, genau f√ºr unklare F√§lle
**Nachteile:** Etwas komplexer

---

## API-Integration {#api-integration}

### ChatGPT API Integration

```python
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

class ChatGPTAssistant:
    def __init__(self):
        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.conversation_history = []
        
    def query(self, user_input):
        """Sendet Anfrage an ChatGPT und gibt Antwort zur√ºck."""
        
        # F√ºge Nutzer-Input zu History hinzu
        self.conversation_history.append({
            "role": "user",
            "content": user_input
        })
        
        # System-Prompt
        messages = [
            {
                "role": "system",
                "content": "Du bist ein hilfreicher deutscher Voice Assistant namens Computer. "
                          "Antworte kurz und pr√§gnant (max 2-3 S√§tze). "
                          "Nutze nat√ºrliche, gesprochene Sprache."
            }
        ] + self.conversation_history
        
        # API-Aufruf
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                max_tokens=150,  # Kurze Antworten f√ºr Voice
                temperature=0.7
            )
            
            answer = response.choices[0].message.content
            
            # F√ºge Antwort zu History hinzu
            self.conversation_history.append({
                "role": "assistant",
                "content": answer
            })
            
            # Limitiere History-L√§nge (letzte 10 Nachrichten)
            if len(self.conversation_history) > 10:
                self.conversation_history = self.conversation_history[-10:]
            
            return answer
            
        except Exception as e:
            print(f"ChatGPT API Error: {e}")
            return "Entschuldigung, ich konnte keine Verbindung herstellen."
```

---

### Perplexity API Integration

```python
import requests
import os

class PerplexityAssistant:
    def __init__(self):
        self.api_key = os.getenv('PERPLEXITY_API_KEY')
        self.url = "https://api.perplexity.ai/chat/completions"
        
    def query(self, user_input):
        """Sendet Recherche-Anfrage an Perplexity."""
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "sonar",
            "messages": [
                {
                    "role": "system",
                    "content": "Du bist ein Recherche-Assistent. "
                              "Gib kurze, faktische Antworten mit Quellen."
                },
                {
                    "role": "user",
                    "content": user_input
                }
            ],
            "max_tokens": 200
        }
        
        try:
            response = requests.post(self.url, headers=headers, json=data)
            response.raise_for_status()
            
            answer = response.json()["choices"][0]["message"]["content"]
            return answer
            
        except Exception as e:
            print(f"Perplexity API Error: {e}")
            return "Entschuldigung, ich konnte keine Informationen finden."
```

---

## Fallback-Strategie {#fallback}

### Multi-Tier Fallback

```python
class LLMRouter:
    def __init__(self):
        self.chatgpt = ChatGPTAssistant()
        self.perplexity = PerplexityAssistant()
        
    def route_query(self, text, intent):
        """Routet Anfrage zum passenden LLM mit Fallback."""
        
        # 1. Bestimme prim√§res LLM
        if intent == "QUESTION":
            # Recherche-Fragen ‚Üí Perplexity
            if any(word in text.lower() for word in ["wetter", "news", "aktuell"]):
                primary = self.perplexity
                fallback = self.chatgpt
            else:
                # Allgemeine Fragen ‚Üí ChatGPT
                primary = self.chatgpt
                fallback = self.perplexity
        else:
            # Konversation ‚Üí ChatGPT
            primary = self.chatgpt
            fallback = None
        
        # 2. Versuche prim√§res LLM
        try:
            answer = primary.query(text)
            if answer and "Entschuldigung" not in answer:
                return answer
        except Exception as e:
            print(f"Primary LLM failed: {e}")
        
        # 3. Fallback zu sekund√§rem LLM
        if fallback:
            try:
                answer = fallback.query(text)
                return answer
            except Exception as e:
                print(f"Fallback LLM failed: {e}")
        
        # 4. Offline-Fallback
        return "Entschuldigung, ich kann momentan keine Verbindung herstellen. Versuche es sp√§ter erneut."
```

---

## Code-Struktur {#code-struktur}

### Integration in Voice Assistant

```python
# In voice_assistant_computer.py

from llm_integration import LLMRouter, classify_input

# Initialisierung
llm_router = LLMRouter()

# In main loop, nach STT:
command = result.get("text", "")

if command:
    print(f"üìù [STT] Erkannt: \"{command}\"")
    
    # Klassifiziere Input
    intent = classify_input(command)
    print(f"üîç [CLASSIFIER] Intent: {intent}")
    
    if intent == "COMMAND":
        # Lokale Befehlsausf√ºhrung
        execute_command(command)
    
    elif intent in ["QUESTION", "CONVERSATION"]:
        # LLM-Anfrage
        print("ü§ñ [LLM] Sende Anfrage...")
        answer = llm_router.route_query(command, intent)
        print(f"üí¨ [LLM] Antwort: {answer}")
        speak(answer)
    
    else:
        speak("Befehl nicht verstanden")
```

---

## Implementierungs-Roadmap {#roadmap}

### Phase 1: Basis-Integration (1-2 Tage)

```
[ ] OpenAI API-Key besorgen
[ ] ChatGPT Integration implementieren
[ ] Einfache Klassifizierung (Pattern-based)
[ ] Testing mit 10 Beispiel-Fragen
[ ] Dokumentation
```

### Phase 2: Erweiterte Features (3-5 Tage)

```
[ ] Perplexity API-Key besorgen
[ ] Perplexity Integration implementieren
[ ] LLM-Router mit Fallback
[ ] Konversations-History
[ ] Hybrid-Klassifizierung
```

### Phase 3: Optimierung (1 Woche)

```
[ ] Latenz-Optimierung (Streaming)
[ ] Caching f√ºr h√§ufige Fragen
[ ] Kosten-Monitoring
[ ] Erweiterte Fehlerbehandlung
[ ] Performance-Tests
```

### Phase 4: Erweiterte LLMs (Optional)

```
[ ] Manus Integration
[ ] Lokales LLM (Ollama) als Offline-Fallback
[ ] Multi-Modal (Bilder, Dokumente)
```

---

**Dokumentende**

---
**Seite {page}**
